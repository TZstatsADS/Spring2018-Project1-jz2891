---
title: 'Project 1: An R Notebook Data Story on Presidential Inaugural Speeches'
output:
  html_document: default
  html_notebook: default
---
\
In this project, we are interested in the difference between presidential inaugural speeches of the two parties - **Democrat** vs **Republican**. The following demonstates a list of Presidents belonging to each party. 
\




```{r, echo = FALSE, message=FALSE, warning=FALSE}

### Step 0: check and install needed packages. Load the libraries and functions. 

packages.used <- c("rvest", "tibble", "qdap", "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", "beeswarm", "scales", "RColorBrewer",
                "RANN", "topicmodels", "ggplot2", "DT", "easyGgplot2")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest") # Web Scraping
library("tibble")

# You may need to run
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib
# in order to load qdap
# 
dyn.load(paste0(system2('/usr/libexec/java_home', stdout = TRUE), '/lib/server/libjvm.dylib'))

library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("ggplot2")
library("DT")
library("easyGgplot2")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

```{r, include = FALSE}
print(R.version)
```


```{r, message=FALSE, warning=FALSE, include=FALSE}

### Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.
# Following the example of [Jerid Francom](https://francojc.github.io/2015/03/01/web-scraping-with-rvest-in-r/), we used [Selectorgadget](http://selectorgadget.com/) to choose the links we would like to scrap. For this project, we selected all inaugural addresses of past presidents.

### Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug <- f.speechlinks(main.page)
# head(inaug)
# as.Date(inaug[,1], format="%B %e, %Y")
inaug <- inaug[-nrow(inaug),] # remove the last line "NA", irrelevant due to error.
# as.Date(inaug[,1], format="%B %e, %Y")
```



```{r, include=FALSE}

### Step 2: Using speech metadata posted on <http://www.presidency.ucsb.edu/>, we prepared CSV data sets for the speeches we will scrap. 

inaug.list <- read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)

# We assemble all scrapped speeches into one list. Note here that we don't have the full text yet, only the links to full text transcripts. 
```


```{r, include=FALSE}

### Step 3: scrap the texts of speeches from the speech URLs.

speech.list <- cbind(inaug.list, inaug)
speech.list$type <- rep("inaug", nrow(inaug.list))

# Based on the list of speeches, we scrap the main text part of the transcript's html page. For simple html pages of this kind,  [Selectorgadget](http://selectorgadget.com/) is very convenient for identifying the html node that `rvest` can use to scrap its content. For reproducibility, we also save our scrapped speeches into our local folder as individual speech files. 
```


```{r, include=FALSE}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/InauguralAddress/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```


```{r, message=FALSE, warning=FALSE, include=FALSE}

### Step 4: data Processing --- generate list of sentences

# We will use sentences as units of analysis for this project, as sentences are natural languge units for organizing thoughts and ideas. For each extracted sentence, we apply sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing."

# We assign an sequential id to each sentence in a speech (`sent.id`) and also calculated the number of words in each sentence as *sentence length* (`word.count`).

sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```


```{r include=FALSE}

# Some non-sentences exist in raw data due to erroneous extra end-of-sentence marks. 

sentence.list <- 
  sentence.list %>%
  filter(!is.na(word.count), Party %in% c("Democratic", "Republican"))

# Split data into two groups

Democratic.sentence.list <- 
  sentence.list %>%
  filter(Party == "Democratic")

Republican.sentence.list <- 
  sentence.list %>%
  filter(Party == "Republican")
```


```{r echo=FALSE}
list(
Republican=unique(Republican.sentence.list$President),
Democratic=unique(Democratic.sentence.list$President)) 
```


### 1. Positive-Negative Clustering

According to sentiment analysis - **positive** vs **negative**, we conduct k-means clustering on the presidential inaugural speeches and thus obtain two clusters. According to the cluster plot, the **positive** sentiment in cluster 2 (blue) is apparently higher than that in cluster 1 (red), while the **negative** sentiment in cluster 2 (blue) is slightly higher than that in cluster 1 (red).

```{r, fig.height=3.3, fig.width=3.7, echo=FALSE}
# Sentiment Analysis
presid.summary <- 
  tbl_df(rbind(Democratic.sentence.list, Republican.sentence.list)) %>%
  filter(type=="inaug") %>%
  #group_by(paste0(type, File))%>%
  group_by(File) %>%
  summarise(
    negative=mean(negative),
    positive=mean(positive)
  )

presid.summary <- as.data.frame(presid.summary)
rownames(presid.summary) <- as.character((presid.summary[,1]))
km.res <- kmeans(presid.summary[,-1], iter.max=100, 2)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="negative", xaxt="n",
             show.clust.cent=FALSE)
```


\
The Presidents in each cluster are listed below. Hence we can calculate the ratio of Presidents in each cluster belonging to each party. For example, the ratio of Presidents in cluster 1 belonging to Republican is 61%.

```{r echo = FALSE}
list(cluster1 = names(km.res$cluster[km.res$cluster == 1]), 
     cluster2 = names(km.res$cluster[km.res$cluster == 2]))
```
\


### 2. Sentiment Flow
Since we have quantified both the positive and negative sentiment in each sentence of inaugural speeches, we are also interested in the sentiment flow during the whole speech. We conduct **positive - negative** to represent the net sentiment.

```{r  fig.width=9, fig.height=10, echo = FALSE, warning=FALSE}
Democratic.sentence.list.sel <- Democratic.sentence.list.sel %>%
  mutate(net_sentiment = round(positive - negative, 6))

ggplot(Democratic.sentence.list.sel, 
       aes(sent.id, net_sentiment, fill = FileOrdered)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~FileOrdered, ncol = 3, scales = "free_x") +
  theme(strip.text = element_text(size=20), plot.title = element_text(size=40)) +
  labs(title="Democratic Presidents") 
```

The top 50 emotionally charged (largest absolute value of net_sentiment) sentences of **Democratic** Presidential inaugural speeches are listed blow.

```{r echo = FALSE}
Democratic.sentence.list.sel %>%
  select(President, sentences, net_sentiment) %>%
  arrange(desc(abs(net_sentiment))) %>%
  head(n=50) %>%
  datatable(options = list(scrollX=T, pageLength = 10))
```

\

```{r fig.width=9, fig.height=10, echo = FALSE}
Republican.sentence.list.sel <- Republican.sentence.list.sel %>%
  mutate(net_sentiment = round(positive - negative, 6))

ggplot(Republican.sentence.list.sel, 
       aes(sent.id, net_sentiment, fill = FileOrdered)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~FileOrdered, ncol = 3, scales = "free_x") +
  theme(strip.text = element_text(size=20), plot.title = element_text(size=40)) +
  labs(title="Republican Presidents") 
```

The top 50 emotionally charged (largest absolute value of net_sentiment) sentences of **Republican** Presidential inaugural speeches are listed blow.

```{r echo = FALSE}
Republican.sentence.list.sel %>%
  select(President, sentences, net_sentiment) %>%
  arrange(desc(abs(net_sentiment))) %>%
  head(n=50) %>%
  datatable(options = list(scrollX=T, pageLength = 10))
```
 
\
 
From the above two lists of emotionally charged (largest absolute value of net_sentiment) sentences, we can easily find out that the inaugural speeches of Republican Presidents seem more emotional since a couple of largest absolute net sentiments are approximately doubled. 

\

We summarize **net_sentiment** for presidential inaugural speeches of both parties. Except for first quarter quantile, all of the quantitative indicators for Republican are larger than that for Democratic, especially the  **Sum(diff)** which represents the summation of first-order derivative of net sentiment. The larger the **Sum(diff)**, the more frequent the emotional variation.

```{r echo = FALSE}
summary.net_sentiment <- rbind(summary(Democratic.sentence.list.sel$net_sentiment),
                               summary(Republican.sentence.list.sel$net_sentiment))
SD.net_sentiment <- c(sd(Democratic.sentence.list.sel$net_sentiment),
                      sd(Republican.sentence.list.sel$net_sentiment))
sum.diff <- c(sum(abs(diff(Democratic.sentence.list.sel$net_sentiment))),
              sum(abs(diff(Republican.sentence.list.sel$net_sentiment))))
mat <- matrix(nrow = 2, ncol = 8)
mat[,1:6] <- summary.net_sentiment
mat[,7] <- SD.net_sentiment
mat[,8] <- sum.diff
colnames(mat) <- c(colnames(summary.net_sentiment), "SD", "Sum(diff)")
rownames(mat) <- c("Democratic", "Republican")
mat 
```

```{r echo = FALSE}
p1 <- ggplot(Democratic.sentence.list.sel) +
  geom_histogram(aes(x = net_sentiment, y = ..density..)) +
  xlim(-0.6, 1.1) + 
  geom_vline(aes(xintercept = mean(positive - negative)), color = "red") +
  labs(title="Democratic Presidents") 

p2 <- ggplot(Republican.sentence.list.sel) +
  geom_histogram(aes(x = net_sentiment, y = ..density..)) +    
  xlim(-0.6, 1.1) +
  geom_vline(aes(xintercept = mean(positive - negative)), color = "red") +
  labs(title="Republican Presidents") 

ggplot2.multiplot(p1, p2, cols=1)
```

